{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#Setting gpu for limit memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    #Restrict Tensorflow to only allocate 6gb of memory on the first GPU\n",
    "   try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6144)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "   except RuntimeError as e:\n",
    "       #virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'Normal_Videos_event', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n"
     ]
    }
   ],
   "source": [
    "base_url = './crime_data/Anomaly-Videos-Part-1/'\n",
    "classes = [i for i in os.listdir(base_url) if not i.startswith('.')]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abuse': 0, 'Arrest': 1, 'Arson': 2, 'Assault': 3, 'Burglary': 4, 'Explosion': 5, 'Fighting': 6, 'Normal_Videos_event': 7, 'RoadAccidents': 8, 'Robbery': 9, 'Shooting': 10, 'Shoplifting': 11, 'Stealing': 12, 'Vandalism': 13}\n"
     ]
    }
   ],
   "source": [
    "class_ids = {}\n",
    "id = 0\n",
    "for c in classes:\n",
    "    class_ids[c] = id\n",
    "    id += 1\n",
    "\n",
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_001 = open('./crime_data/UCF_Crimes-Train-Test-Split/Action_Regnition_splits/train_001.txt','r').read().split('\\n')\n",
    "len(train_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for i in train_001:\n",
    "    x = i.split('/')[0]\n",
    "    if x not in list(counts.keys()):\n",
    "        counts[x] = 1\n",
    "    else:\n",
    "        counts[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abuse': 38,\n",
       " 'Arrest': 38,\n",
       " 'Arson': 38,\n",
       " 'Assault': 38,\n",
       " 'Burglary': 38,\n",
       " 'Explosion': 38,\n",
       " 'Fighting': 38,\n",
       " 'Normal_Videos_event': 38,\n",
       " 'RoadAccidents': 38,\n",
       " 'Robbery': 38,\n",
       " 'Shooting': 38,\n",
       " 'Shoplifting': 38,\n",
       " 'Stealing': 38,\n",
       " 'Vandalism': 38}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_urls = []\n",
    "val_counts = {}\n",
    "train_urls = []\n",
    "train_counts = {}\n",
    "for i in train_001:\n",
    "    x = i.split('/')[0]\n",
    "    if x not in list(val_counts.keys()):\n",
    "        val_counts[x] = 1\n",
    "        val_urls.append(i)\n",
    "    elif val_counts[x] < 4 :\n",
    "        val_counts[x] += 1\n",
    "        val_urls.append(i)\n",
    "    else:\n",
    "        if x not in list(train_counts.keys()):\n",
    "            train_counts[x] = 1\n",
    "            train_urls.append(i)\n",
    "        else:\n",
    "            train_counts[x] += 1\n",
    "            train_urls.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abuse': 4,\n",
       " 'Arrest': 4,\n",
       " 'Arson': 4,\n",
       " 'Assault': 4,\n",
       " 'Burglary': 4,\n",
       " 'Explosion': 4,\n",
       " 'Fighting': 4,\n",
       " 'Normal_Videos_event': 4,\n",
       " 'RoadAccidents': 4,\n",
       " 'Robbery': 4,\n",
       " 'Shooting': 4,\n",
       " 'Shoplifting': 4,\n",
       " 'Stealing': 4,\n",
       " 'Vandalism': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abuse': 34,\n",
       " 'Arrest': 34,\n",
       " 'Arson': 34,\n",
       " 'Assault': 34,\n",
       " 'Burglary': 34,\n",
       " 'Explosion': 34,\n",
       " 'Fighting': 34,\n",
       " 'Normal_Videos_event': 34,\n",
       " 'RoadAccidents': 34,\n",
       " 'Robbery': 34,\n",
       " 'Shooting': 34,\n",
       " 'Shoplifting': 34,\n",
       " 'Stealing': 34,\n",
       " 'Vandalism': 34}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_urls)\n",
    "random.shuffle(val_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_urls = open('./crime_data/UCF_Crimes-Train-Test-Split/Action_Regnition_splits/test_001.txt','r').read().split(' \\n')\n",
    "len(test_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nframes = 10\n",
    "batch_size = 8\n",
    "HEIGHT = 240\n",
    "WIDTH = 320\n",
    "def format_frames(frame):\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_crop_or_pad(frame, HEIGHT, WIDTH)\n",
    "    return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (HEIGHT,WIDTH), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result\n",
    "\n",
    "class FrameGenerator:\n",
    "  def __init__(self,paths, n_frames,testing = False):\n",
    "    self.n_frames = n_frames\n",
    "    self.paths = paths\n",
    "    self.testing = testing\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    for path in self.paths:\n",
    "      video_paths.append(f'{base_url}{path}'.strip())\n",
    "      labels.append(path.split('/')[0])\n",
    "    pairs = list(zip(video_paths,labels))\n",
    "    return pairs\n",
    "\n",
    "  def __call__(self):\n",
    "    pairs = self.get_files_and_class_names()\n",
    "\n",
    "    if not self.testing:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames)\n",
    "      label = class_ids[name]\n",
    "      #yield video_frames, to_categorical(label, len(classes))\n",
    "      yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "\n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "\n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "\n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(paths=train_urls,n_frames=nframes),\n",
    "                                          output_signature = output_signature)\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(paths=val_urls,n_frames=nframes),\n",
    "                                          output_signature = output_signature)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, nframes, HEIGHT, WIDTH, 3)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(14)(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint_path = './model/checkpoints/checkpoint.model01_weights.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, monitor=\"val_accuracy\", mode=\"max\",save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 262s 4s/step - loss: 2.6567 - accuracy: 0.1008 - val_loss: 2.9732 - val_accuracy: 0.1071\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 236s 4s/step - loss: 2.6137 - accuracy: 0.1218 - val_loss: 2.9101 - val_accuracy: 0.0536\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5949 - accuracy: 0.0945 - val_loss: 2.8947 - val_accuracy: 0.0536\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5809 - accuracy: 0.1366 - val_loss: 2.8904 - val_accuracy: 0.0714\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5513 - accuracy: 0.1239 - val_loss: 3.1246 - val_accuracy: 0.0536\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5801 - accuracy: 0.1176 - val_loss: 2.6275 - val_accuracy: 0.1250\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5249 - accuracy: 0.1324 - val_loss: 2.7161 - val_accuracy: 0.0714\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5516 - accuracy: 0.1303 - val_loss: 2.6222 - val_accuracy: 0.1429\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5184 - accuracy: 0.1366 - val_loss: 2.7072 - val_accuracy: 0.0893\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.5064 - accuracy: 0.1555 - val_loss: 2.5773 - val_accuracy: 0.0714\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 216s 4s/step - loss: 2.5011 - accuracy: 0.1534 - val_loss: 2.6675 - val_accuracy: 0.1071\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4766 - accuracy: 0.1471 - val_loss: 2.6907 - val_accuracy: 0.0714\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4660 - accuracy: 0.1702 - val_loss: 2.7194 - val_accuracy: 0.1786\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4331 - accuracy: 0.1828 - val_loss: 2.5280 - val_accuracy: 0.1607\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 216s 4s/step - loss: 2.4627 - accuracy: 0.1870 - val_loss: 2.8916 - val_accuracy: 0.0893\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4740 - accuracy: 0.1408 - val_loss: 3.3218 - val_accuracy: 0.0714\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4993 - accuracy: 0.1303 - val_loss: 2.7670 - val_accuracy: 0.1429\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4700 - accuracy: 0.1618 - val_loss: 2.6047 - val_accuracy: 0.1786\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4360 - accuracy: 0.1534 - val_loss: 2.5438 - val_accuracy: 0.1250\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 221s 4s/step - loss: 2.4686 - accuracy: 0.1723 - val_loss: 2.7150 - val_accuracy: 0.1786\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 217s 4s/step - loss: 2.4338 - accuracy: 0.1765 - val_loss: 2.5924 - val_accuracy: 0.1250\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4188 - accuracy: 0.1639 - val_loss: 2.9021 - val_accuracy: 0.0714\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4163 - accuracy: 0.2017 - val_loss: 2.5031 - val_accuracy: 0.1250\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4102 - accuracy: 0.1786 - val_loss: 2.6624 - val_accuracy: 0.1429\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4399 - accuracy: 0.1786 - val_loss: 2.5413 - val_accuracy: 0.1071\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4068 - accuracy: 0.1828 - val_loss: 2.4917 - val_accuracy: 0.1250\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.4258 - accuracy: 0.1765 - val_loss: 2.5382 - val_accuracy: 0.0714\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3984 - accuracy: 0.1912 - val_loss: 2.4921 - val_accuracy: 0.0893\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3461 - accuracy: 0.1933 - val_loss: 2.7015 - val_accuracy: 0.1607\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3729 - accuracy: 0.2122 - val_loss: 2.8108 - val_accuracy: 0.1607\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3818 - accuracy: 0.2101 - val_loss: 2.5503 - val_accuracy: 0.1607\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3621 - accuracy: 0.2269 - val_loss: 3.1398 - val_accuracy: 0.1071\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 214s 4s/step - loss: 2.3770 - accuracy: 0.1870 - val_loss: 2.9917 - val_accuracy: 0.1429\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3678 - accuracy: 0.1870 - val_loss: 2.7232 - val_accuracy: 0.1429\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3375 - accuracy: 0.1954 - val_loss: 2.5545 - val_accuracy: 0.1071\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3623 - accuracy: 0.2038 - val_loss: 2.4843 - val_accuracy: 0.1071\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3560 - accuracy: 0.2059 - val_loss: 2.6049 - val_accuracy: 0.0714\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3338 - accuracy: 0.2017 - val_loss: 2.6551 - val_accuracy: 0.1429\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3279 - accuracy: 0.2206 - val_loss: 2.5702 - val_accuracy: 0.1250\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3276 - accuracy: 0.2185 - val_loss: 2.7552 - val_accuracy: 0.0357\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 214s 4s/step - loss: 2.3201 - accuracy: 0.2227 - val_loss: 2.7006 - val_accuracy: 0.0714\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3205 - accuracy: 0.2206 - val_loss: 2.6052 - val_accuracy: 0.1071\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3300 - accuracy: 0.2332 - val_loss: 2.6544 - val_accuracy: 0.1071\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.2970 - accuracy: 0.2164 - val_loss: 2.5276 - val_accuracy: 0.0893\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.2851 - accuracy: 0.2374 - val_loss: 2.5199 - val_accuracy: 0.1250\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3430 - accuracy: 0.2395 - val_loss: 2.7415 - val_accuracy: 0.1250\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3253 - accuracy: 0.2185 - val_loss: 2.8709 - val_accuracy: 0.1429\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3367 - accuracy: 0.2185 - val_loss: 2.9915 - val_accuracy: 0.1071\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.3227 - accuracy: 0.2353 - val_loss: 2.6590 - val_accuracy: 0.1429\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 215s 4s/step - loss: 2.2906 - accuracy: 0.2311 - val_loss: 2.6601 - val_accuracy: 0.1607\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 50, \n",
    "                    validation_data = val_ds,\n",
    "                    callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 19s 868ms/step - loss: 2.6951 - accuracy: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6950976848602295, 0.125]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(test_urls,nframes,testing=True),output_signature=output_signature).batch(batch_size)\n",
    "model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-metal-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
